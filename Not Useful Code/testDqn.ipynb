{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b485ff32",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.results_plotter import load_results, plot_results\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "\n",
    "# --- Configuration ---\n",
    "ENV_IDS = [\"CartPole-v1\", \"LunarLander-v2\"]  # Multiple different Gym environments\n",
    "NUM_ENVS_PER_PROBLEM = 4  # Number of parallel environments for each problem\n",
    "TOTAL_TIMESTEPS_PER_PROBLEM = 50_000  # Total training timesteps for each environment\n",
    "LOG_DIR = \"dqn_multi_env_logs\"\n",
    "EVAL_FREQ = 1000  # Evaluate every EVAL_FREQ timesteps\n",
    "N_EVAL_EPISODES = 10  # Number of episodes for evaluation\n",
    "\n",
    "# --- Create logging directory ---\n",
    "os.makedirs(LOG_DIR, exist_ok=True)\n",
    "\n",
    "# --- Custom Callback for saving best model and logging rewards ---\n",
    "class EvalCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that extends the BaseCallback from Stable-Baselines3.\n",
    "    It evaluates the agent periodically and saves the best model.\n",
    "    \"\"\"\n",
    "    def __init__(self, eval_env, eval_freq, log_dir, n_eval_episodes=5, verbose=1):\n",
    "        super().__init__(verbose)\n",
    "        self.eval_env = eval_env\n",
    "        self.eval_freq = eval_freq\n",
    "        self.log_dir = log_dir\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.episode_rewards = []\n",
    "        self.timesteps = []\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.eval_freq == 0:\n",
    "            mean_reward, _ = self.evaluate_agent()\n",
    "            self.episode_rewards.append(mean_reward)\n",
    "            self.timesteps.append(self.num_timesteps)\n",
    "\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                if self.verbose > 0:\n",
    "                    print(f\"New best mean reward: {mean_reward:.2f}, saving model.\")\n",
    "                self.best_mean_reward = mean_reward\n",
    "                self.model.save(os.path.join(self.log_dir, \"best_model\"))\n",
    "        return True\n",
    "\n",
    "    def evaluate_agent(self):\n",
    "        \"\"\"\n",
    "        Evaluate the agent's performance.\n",
    "        \"\"\"\n",
    "        all_episode_rewards = []\n",
    "        for _ in range(self.n_eval_episodes):\n",
    "            obs, _ = self.eval_env.reset()\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "            while not done:\n",
    "                action, _ = self.model.predict(obs, deterministic=True)\n",
    "                obs, reward, terminated, truncated, _ = self.eval_env.step(action)\n",
    "                done = terminated or truncated\n",
    "                episode_reward += reward\n",
    "            all_episode_rewards.append(episode_reward)\n",
    "        mean_reward = np.mean(all_episode_rewards)\n",
    "        std_reward = np.std(all_episode_rewards)\n",
    "        return mean_reward, std_reward\n",
    "\n",
    "# --- Training and Evaluation Loop ---\n",
    "results = {}\n",
    "\n",
    "for env_id in ENV_IDS:\n",
    "    print(f\"\\n--- Training DQN on {env_id} ---\")\n",
    "    \n",
    "    # Create vectorized environments for training\n",
    "    # Use SubprocVecEnv for parallel execution, especially for more complex environments\n",
    "    # For simpler environments like CartPole, DummyVecEnv might be faster due to less overhead\n",
    "    \n",
    "    # Create a function that returns a new environment for make_vec_env\n",
    "    def make_env():\n",
    "        # Monitor wrapper logs training statistics to a CSV file\n",
    "        return Monitor(gym.make(env_id)) \n",
    "\n",
    "    # For SubprocVecEnv, it's recommended to wrap the environment creation in a function\n",
    "    # and use make_vec_env with vec_env_cls=SubprocVecEnv\n",
    "    train_env = make_vec_env(make_env, n_envs=NUM_ENVS_PER_PROBLEM, vec_env_cls=SubprocVecEnv)\n",
    "    \n",
    "    # Create a separate evaluation environment (single, not vectorized for clear episode results)\n",
    "    eval_env = Monitor(gym.make(env_id))\n",
    "\n",
    "    # Instantiate the DQN agent\n",
    "    model = DQN(\n",
    "        \"MlpPolicy\",\n",
    "        train_env,\n",
    "        learning_rate=1e-4,\n",
    "        buffer_size=100000,\n",
    "        learning_starts=1000,\n",
    "        batch_size=32,\n",
    "        gamma=0.99,\n",
    "        train_freq=4,\n",
    "        gradient_steps=1,\n",
    "        target_update_interval=1000,\n",
    "        exploration_fraction=0.1,\n",
    "        exploration_initial_eps=1.0,\n",
    "        exploration_final_eps=0.05,\n",
    "        verbose=0,\n",
    "        tensorboard_log=LOG_DIR,\n",
    "    )\n",
    "\n",
    "    # Create the custom evaluation callback\n",
    "    callback = EvalCallback(eval_env, EVAL_FREQ, os.path.join(LOG_DIR, env_id), N_EVAL_EPISODES)\n",
    "\n",
    "    # Train the agent\n",
    "    model.learn(total_timesteps=TOTAL_TIMESTEPS_PER_PROBLEM, callback=callback, progress_bar=True)\n",
    "    \n",
    "    # Store the results from the callback\n",
    "    results[env_id] = {\n",
    "        'timesteps': callback.timesteps,\n",
    "        'rewards': callback.episode_rewards\n",
    "    }\n",
    "\n",
    "    # Close environments\n",
    "    train_env.close()\n",
    "    eval_env.close()\n",
    "\n",
    "# --- Plotting Median Reward ---\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "for env_id, data in results.items():\n",
    "    timesteps = np.array(data['timesteps'])\n",
    "    rewards = np.array(data['rewards'])\n",
    "    \n",
    "    # We already have the evaluation mean rewards from the EvalCallback,\n",
    "    # which is generally what people plot for performance over time.\n",
    "    # If you strictly want *median* reward per episode during training,\n",
    "    # you would need to modify the Monitor callback or collect full episode rewards\n",
    "    # and then calculate the median over a window.\n",
    "    # However, for plotting performance during learning, mean reward is standard.\n",
    "    # The EvalCallback already provides mean reward over N_EVAL_EPISODES.\n",
    "    \n",
    "    plt.plot(timesteps, rewards, label=f'{env_id} Mean Evaluation Reward')\n",
    "\n",
    "plt.xlabel(\"Timesteps\")\n",
    "plt.ylabel(\"Mean Episode Reward (Evaluation)\")\n",
    "plt.title(\"DQN Training Performance Across Different Environments\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# You can also use Stable-Baselines3's built-in plotter for training rewards (from Monitor)\n",
    "# This plots the mean reward over a rolling window.\n",
    "print(\"\\nPlotting training results using Stable-Baselines3's results plotter:\")\n",
    "for env_id in ENV_IDS:\n",
    "    # The Monitor wrapper saves logs to a CSV file.\n",
    "    # We need to load them to plot the training curve.\n",
    "    log_path = os.path.join(LOG_DIR, env_id)\n",
    "    if os.path.exists(log_path):\n",
    "        x, y = ts2xy(load_results(log_path), 'timesteps')\n",
    "        if len(x) > 0:\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            plt.plot(x, y)\n",
    "            plt.xlabel(\"Timesteps\")\n",
    "            plt.ylabel(\"Mean Reward (Training)\")\n",
    "            plt.title(f\"DQN Training Curve for {env_id}\")\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            print(f\"No training logs found for {env_id} at {log_path}\")\n",
    "    else:\n",
    "        print(f\"Log directory for {env_id} not found at {log_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogsat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
